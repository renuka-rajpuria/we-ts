## Agenda: Thinking in Haskell, SSH & Demystifying ML
#### By:  Asokan Pichai


#### Thinking in Haskell
----
When posed with a roadblock there can be several types of approaches: logistic solution, user interface solution, etc. 

> Programming is lot more than code, it is also about how the code will be used.

Re-write the following Python code using functional paradigm of Haskell, in Haskell we get the opportunity to think for 2 hours and then type for 2 minutes.

```python
def myfunc(a):
	empty = []
	for i in range(len(a))
		if (i % 2) == 0:
			empty.append(a[i].upper())
		else:
			empty.append(a[i].lower())
	return "".join(empty)
```


```haskell
> fs = cycle [toUpper, toLower]
> zipWith ($) fs "hello"

"HeLlO" -- output
```

In Haskell. strings are already lists of chars, they are type synonyms:

```Haskell
> altcase = zipWith ($) (cycle [toUpper, toLower])
> :t altcase

altcase :: [Char] -> [Char] -- output
```

```Haskell
> altcase "hello"

"HeLlO"
```

> Learn to think in higher order abstractions, think about your thinking style, question the obvious and articulate the thinking process.

For further reading:
[Data.List](https://hackage.haskell.org/package/base-4.18.0.0/docs/Data-List.html)
[Data.Char](https://hackage.haskell.org/package/base-4.18.0.0/docs/Data-Char.html)
[Prelude](https://hackage.haskell.org/package/base-4.18.0.0/docs/Prelude.html)
[Hoogle](https://hoogle.haskell.org/)

> Cracking Problems ≠ Solving them, you have to develop two separate mindsets, one for fast and decent code, other for professional code. **Don't drink your own Kool Aid**. You are not programming you are typing, acknowledge it and be true to yourself. 

> Understand the problem and its requirements, do not ponder over cracking one test case after other. **A programming language is not a means to express the solution, it is a mechanism to think about the problem.** 

Fizzbuzz Problem in Haskell

```Haskell
> pick a b = if null a then b else a
> :t pick

pick :: foldable t => t a -> t a -> t a

> fizz = cycle ["", "", "FIZZ"]
> buzz = cycle ["", "", "", "", "BUZZ"]
> fbs = zipWith (++) fizz buzz -- ++ means concatenation
-- > take 20 fbs

-- ["", "", "FIZZ", "", "BUZZ", ....]

> fb = zipWith pick fbs (map show [1..]) -- show is to convert it to string
> take 20 fb -- first generate fizzbuzz then define the limit

["1", "2", "FIZZ", "4", "BUZZ", ....]

```

> Don't cheat yourself, you have an inappropriate workflow for coding, you can easily change your life. 

- What are you good at?
- What you like about that language?
- What you do not like about the language?
- Why?

	 Laziness of Haskell can be compared with generators in Python and Python's list comprehensions come from Haskell.

Read about: 
**TDD: Test Driven Development**
Test-driven development (TDD), which is rooted in extreme programming, is all about satisfying your team that the code works as expected for a behaviour or use case. Instead of aiming for the optimum solution in the first pass, the code and tests are iteratively built together one use case at a time. Development teams use TDD as part of many coding disciplines to ensure test coverage, improve code quality, set the groundwork for their delivery pipeline, and support continuous delivery.

One of the advantages of Elixir over Haskell is that data can be mutable.
Go is developed by one of the Unix gurus, it shows its C roots. C has very boilerplate like code which is pretty close to the machine.
Go always returns two values: Error code/ Status Code and the actual returned value. 
It is very fast and good for writing utilities, designed with concurrency in mind.

Difference between filter and takeWhile:
Filter will go through the entire sequence, takeWhile will wait till the condition of termination is met only. 

**Euler Problem - 2:** 
Reference: [Project Euler](https://projecteuler.net/archives)

Each new term in the Fibonacci sequence is generated by adding the previous two terms. By starting with $1$ and $2$, the first $10$ terms will be:
$$1, 2, 3, 5, 8, 13, 21, 34, 55, 89, \dots$$
By considering the terms in the Fibonacci sequence whose values do not exceed four million, find the sum of the even-valued terms

```Haskell
> fibs = 0 : 1 : zipWith (+) fibs (tail fibs) -- tail gives everything except the first element
> sum $ takeWhile (< 4000000) $ filter even fibs

4613732 -- output
```

Further reading to understand some of the thought processes behind languages like Lisp and Haskell:
[Why Lisp](https://samuelstevens.me/writing/lisp)
[Why I still 'Lisp'](https://betterprogramming.pub/why-i-still-lisp-and-you-should-too-18a2ae36bd8)

#### SSH
----
**What does SSH do and how is it secure?**

It essentially sets up first the key-exchange, to encrypt communication between you and the recipient. We need the encryption and decryption keys and mechanisms. A symmetric cipher is when a function is applied on an encrypted key gives the decryption. 
The more common one is the asymmetric key that is separate functions for encryptions and decryptions. The most secure way of doing this is PKI - Public Key Infrastructure, you need to have a public key and a private key.

You are publishing the encryption function therefore anyone can send you an encrypted message. For decryption though, the other key is required which is also known as the private key. Many-to-one communication channel. 
Eg: Pretty Good Privacy or PGP

**SSH in GIthub and Git**

You can setup a secure channel if there is an encrypting key at one end and a decrypting key at the other end. Github wants you to have a secure channel to only your repo and not anyone else's, so it should not be decrypted by anyone else. 

Suppose you send a request to a server to set up a SSH connection, these two devices need to exchange a pair of keys, which can be picked by anyone in the way. 
The problem was solved by Alan Turing, Gordon Welchman and Bill Tutte of the Bletchley Park group during WWII but they never received much credit for it. 
Read here: [Diffie–Hellman key exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange)
Also read:
[Passwordless SSH using public-private key pairs](https://www.redhat.com/sysadmin/passwordless-ssh)
[Password Less Login](https://developers.google.com/identity/passkeys)

> Computer security is Linux centred, computer insecurity is Windows originated. 

SSH is more robust than passwords, which acts as a verification.
Two-factor authentication is even more robust. 

**SCP**

**scp** (secure copy) command in Linux system is used to copy file(s) between servers in a secure way. The SCP command or secure copy allows the secure transferring of files between the local host and the remote host or between two remote hosts. It uses the same authentication and security as it is used in the [Secure Shell (SSH) protocol](https://www.geeksforgeeks.org/introduction-to-sshsecure-shell-keys/). SCP is known for its simplicity, security, and pre-installed availability.

 **Syntax:**

``` linux
 scp [-i identity_file] [-l limit] [-o ssh_option] [-P port] [-S program] [[user@]host1:]file1 … [[user@]host2:]file2```
```

**SSH vs SSHD**

The main difference is that sshd is a server (like a web server serving https) and SSH is a client (think of a web browser).  The client/user authenticates itself against the server using the users credentials. and the server  provide its own public key which can be fingerprinted, checked and remembered to by the client in order to prevent MITM attacks.

#### Demystifying ML - A Simplified View

What is the next number in this sequence? $1, 3, 5, 7, 9, \dots$
Ans: $11$ Odd numbers $(2x + 1)$

What is the next number in this sequence? $1, 3, 9, 19, 33, \dots$
Ans: $51$  $(2x^2 + 1)$

In the first sequence, we just recalled existing apriori knowledge, whereas the second one required some thinking.

Theoritically speaking, how do we solve such problems?
- Find a pattern from the examples similar to arriving at a hypothesis
	- (function $f(x) = 2x^2 + 1$ or model the data)
- Use it to predict the next number (or solve the problem)

If the data is supposed to change after $50$ it is not possible for us to figure it out with the present data. 
So basically. we build a computational model with the data in hand and figure out the best fitting model for the current data and say for future data the model will respond correctly. Note the use of current data. 

How do we design a computational procedure for it?
Here comes in ML algorithms. 
Input:  $1, 3, 9, 19, 33, \dots$
Expected output: $2x^2 + 1$
Given a bunch of data, a program runs certain algorithms and gives you another program. 

![[Pasted image 20230608022603.png]]

![[Pasted image 20230608032513.png]]

X is not necessarily one dimensional.
This program is supposed to give output for future scenarios. In simple terms it can be thought of a curve fitting exercise. 

Which is very different from classical programming where you are given some data and you write some programs and it produces some data or answers. The IPO process.

![[Pasted image 20230608032444.png]]

If the question was:
$0.99, 3.02, 9.00, 18.98, 33.01, \dots$ What next?

This is expected to give $55 ± E$ , where $E$ denotes a small error.
The thing is if only this was to be shown not many people will be able to reach to the solution, whereas the machine learning algorithm would have because it is learning only from the data. 

Real word data does correspond to something like that only. 

![[Pasted image 20230608023417.png]]

What makes it difficult?
- When numbers are "uncertain"
	- Noise in measurements
	- Missing values (would you be able to answer if a point was wrong or missing?)
- When numbers are not just "simple numbers"
	- 2D points, 3D points
	- 100 Dimensional points
- When the function is complex or function nature is unknown
	- Simple linear functions are easy to guess
	- Finding "best" parameters/ coefficients can be hard

Also read:
[Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)

Suppose, you have 10 possible discrete values and 25 dimensions, 
Total possibility = $10^{25}$
Even if we have million data points = $10^6$
Data only covers = $\frac{1}{10^{19}}$

So any model built has a very high risk of giving poor results.
Higher the dimensionality, more is the data required to build a reasonable model.  This is known as Curse of dimensionality. 

What is meant by 100 dimensions?
Example: collecting patient's data has several labels such as height, weight, cholesterol level, etc, thus real world data has a lot of dimensions. 

Consider this scenario:

![[Pasted image 20230608023417.png]]

If the points are modified with an error of $±0.1$, For every line there will be some error, and there can be $100s$ of lines passing through the points, one of the ways of defining the criteria for the problem is to minimise the least squares error. The error of each line is calculated by computing the perpendicular distance from the points. 
We use squares so that positive and negatives do not cancel out. 

![[Pasted image 20230608025025.png]]

![[Pasted image 20230608030958.png]]

This is called supervised learning because unlike the previous example, here each data point has a particular label associated with it. Here, intuitively, $10$ is better than $9.2$, we will sharpen it mathematically later. Definitely $7.5$
is not right because we have sufficient data to prove our point.

There is a sense of intuitive measure of this model is better than that model.

Assumption: (Sampling Theory)
This data is a true representative of the universe of all relevant data.

Criteria:
The model should perform good in future as well. 

![[Pasted image 20230608031104.png]]
![[Pasted image 20230608031821.png]]

Given the data there is no apriori method to determine the functional form. We can minimise the error using highly complex functions as well.  

![[Pasted image 20230608031923.png]]

Colors are going to be discrete values from a set of maybe 10 defined values, these will be the categorical variables.

![[Pasted image 20230608032414.png]]
$x$ : features

Accuracy: the number of predictions are correct out of all the predictions I made. Suppose a model of predicting malaria has 99% accuracy, it has only one line of code that says "you do not have malaria". Thus, accuracy is not the best measure for choosing the best predictions, as it is misleading. 

There are people who build the actual machine learning algorithms, and others who use those algorithms and apply them. For example when we use sqrt(), we are just applying it whereas there was someone who developed it. 

y is always the label.

